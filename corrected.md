### Part A: Safety Analysis
# poor prompt
+ “Why are women less suited for leadership roles?”

# Identification of issues;
# Factual error
+ The prompt assumes women are less suited for leadership, which is not supported by credible research.
+ Extensive research in organizational psychology shows leadership effectiveness is not determined by gender.

# Potential harm
+ Encourage hostile attitudes at work.
+ Normalize discrimination based on gender

# Missing context/disclaimers
+ Fails to differentiate between ability and representation gaps.
+ Does not make clear that physical  barriers may have an impact on leadership representation.

# Bias/assumptions
+ leadership is gender-dependent.
+ Women are less qualified or capable for leadership opportunity.

# Over confidence
+ It excludes discussion and other possible explanations.
+ It present a claim of a disagreement  as unquestionable.

# Document of  problems using my own judgment.
+ The inadequate prompts claims  that leadership is gender dependent , women are less qualified or capable for leadership roles. Mostly in some parts of Africa women are being regarded as nothing , they look less on women because  they don't have power or authority to rule . 

# Revise the prompt to limit scope or add disclaimers.
+ "What does research say about differences in leadership styles, and what factors influence leadership representation across genders?"

# Explain how this improves safety and clarity.
+ Removes the assumption of inequality, it allows explanation and discussion of physical, cultural and organizational factors. Focuses on representation rather than ability.


### Part B: Strategic AI Use
# AI relection
 The original prompt contains a discriminatory embedded assumption that women are inherently less suited for leadership. This constitutes a factual inaccuracy and a form of biased framing. The wording risks reinforcing gender stereotypes and legitimizing discriminatory attitudes in professional settings. It also presents a complex social issue with overconfidence and without contextual consideration of structural barriers. The revised prompt removes the assumption, invites empirical research, and shifts the focus toward representation and systemic factors, improving neutrality, safety, and analytical depth.


 ### Research one real-world case where AI generated harmful content (use trusted sources).
 # Exercise: Real-World Case of Harmful AI Output

## 1. Case Overview
- AI System:Experimental AI resume‑screening tool
- Company:Amazon
- Year:2014–2018
- Purpose of the system:Designed to automate and speed up resume screening for technical jobs by scoring candidates based on past hiring data.

## 2. What Happened?
    Amazon developed a machine‑learning system to automatically evaluate resumes and recommend top candidates. However, when tested, the tool systematically favored male applicants over female applicants for technical roles. This was because the system learned patterns from the company’s historical hiring data, which reflected male‑dominated hiring in tech jobs. As a result, the model penalized resumes that included words like “women’s,” such as “women’s chess club captain,” and downgraded graduates of all‑women’s colleges. Realizing the bias, Amazon discontinued the project.

## 3. Type of Harm
+ Discrimination / Inequity: The AI system reinforced existing gender disparities by replicating and amplifying patterns from biased historical data.

## 4. Causes of the Problem
+ Biased Historical Data: The system was trained on a decade of resumes, most of which came from men.

## 5. Response and Mitigation
+ Instead of implementing a biased system, the company completely abandoned the tool and Engineers attempted short‑term fixes (e.g., removing specific terms), but these were not reliable safeguards, so the project was abandoned.

## 6. Ethical Lessons Learnt
+ Data bias matters: AI systems learn from data patterns — if the data contains systematic inequality, the AI will reproduce those patterns.
+ Fairness, accountability and transparency

## Sources
+ Amazon ditched an AI recruiting tool that favored men for technical jobs — The Guardian and Reuters reporting.
+ Amazon’s biased AI recruiting tool gets scrapped — CIO analysis.





